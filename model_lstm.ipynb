{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model-lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SnFyCZq1dN2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2016 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"\n",
        "  AutoNLP datasets.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class AutoNLPDataset(object):\n",
        "    def __init__(self, dataset_dir):\n",
        "        \"\"\"\n",
        "            train_dataset, test_dataset: list of strings\n",
        "            train_label: np.array\n",
        "        \"\"\"\n",
        "        self.dataset_name_ = dataset_dir\n",
        "        self.dataset_dir_ = dataset_dir\n",
        "        self.metadata_ = self.read_metadata(os.path.join(dataset_dir, \"meta.json\"))\n",
        "\n",
        "    def read_dataset(self):\n",
        "        self.train_dataset = self._read_dataset(os.path.join(self.dataset_dir_, \"train.data\"))\n",
        "        self.train_label = self.read_label(os.path.join(self.dataset_dir_, \"train.solution\"))\n",
        "        self.test_dataset = self._read_dataset(os.path.join(self.dataset_dir_, \"test.data\"))\n",
        "\n",
        "    def get_train(self):\n",
        "        return self.train_dataset, self.train_label\n",
        "\n",
        "    def get_test(self):\n",
        "        return self.test_dataset\n",
        "\n",
        "    def get_metadata(self):\n",
        "        return self.metadata_\n",
        "\n",
        "    def read_metadata(self, metadata_path):\n",
        "        import json\n",
        "        return json.load(open(metadata_path))\n",
        "\n",
        "    def _read_dataset(self, dataset_path):\n",
        "        with open(dataset_path) as fin:\n",
        "            return fin.readlines()\n",
        "\n",
        "    def read_label(self, label_path):\n",
        "        return np.loadtxt(label_path)\n",
        "\n",
        "    def get_class_num(self):\n",
        "        \"\"\" return the number of class \"\"\"\n",
        "        return self.metadata_[\"class_num\"]\n",
        "\n",
        "    def get_train_num(self):\n",
        "        \"\"\" return the number of train instance \"\"\"\n",
        "        return self.metadata_[\"train_num\"]\n",
        "\n",
        "    def get_test_num(self):\n",
        "        \"\"\" return the number of test instance \"\"\"\n",
        "        return self.metadata_[\"test_num\"]\n",
        "\n",
        "    def get_language(self):\n",
        "        \"\"\" ZH or EN \"\"\"\n",
        "        return self.metadata_[\"language\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cESc9hfx1jZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypv4TCxn1oIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c9d1628-368b-4235-ec40-65513bc3572c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z71fRBt11pJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = '/content/drive/My Drive/autonlp_starting_kit-master/offline_data/O1/O1.data'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW19Gnax2Wx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoDaset = AutoNLPDataset(file_path)\n",
        "autoDaset.read_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYjf8PNY2Zri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d41b042b-7896-404f-dc24-1f7685b58df8"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import argparse\n",
        "import time\n",
        "import jieba\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys, getopt\n",
        "from subprocess import check_output\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "from tensorflow.python.keras.layers import Dropout\n",
        "from tensorflow.python.keras.layers import Embedding\n",
        "from tensorflow.python.keras.layers import SeparableConv1D\n",
        "from tensorflow.python.keras.layers import SpatialDropout1D\n",
        "from tensorflow.python.keras.layers import LSTM\n",
        "from tensorflow.python.keras.layers import MaxPooling1D\n",
        "from tensorflow.python.keras.layers import MaxPooling2D\n",
        "from tensorflow.python.keras.layers import Flatten\n",
        "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from tensorflow.python.keras.preprocessing import text\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
        "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
        "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
        "sess = tf.Session(config=config)\n",
        "set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
        "\n",
        "\n",
        "MAX_SEQ_LENGTH = 500\n",
        "MAX_VOCAB_SIZE = 20000 # Limit on the number of features. We use the top 20K features\n",
        "\n",
        "# code form https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
        "def clean_en_text(dat):\n",
        "    \n",
        "    REPLACE_BY_SPACE_RE = re.compile('[\"/(){}\\[\\]\\|@,;]')\n",
        "    BAD_SYMBOLS_RE = re.compile('[^0-9a-zA-Z #+_]')\n",
        "    \n",
        "    ret = []\n",
        "    for line in dat:\n",
        "        # text = text.lower() # lowercase text\n",
        "        line = REPLACE_BY_SPACE_RE.sub(' ', line)\n",
        "        line = BAD_SYMBOLS_RE.sub('', line)\n",
        "        line = line.strip()\n",
        "        ret.append(line)\n",
        "    return ret\n",
        "\n",
        "def clean_zh_text(dat):\n",
        "    REPLACE_BY_SPACE_RE = re.compile('[“”【】/（）：！～「」、|，；。\"/(){}\\[\\]\\|@,\\.;]')\n",
        "    \n",
        "    ret = []\n",
        "    for line in dat:\n",
        "        line = REPLACE_BY_SPACE_RE.sub(' ', line)\n",
        "        line = line.strip()\n",
        "        ret.append(line)\n",
        "    return ret\n",
        "\n",
        "\n",
        "def sequentialize_data(train_contents, val_contents=None):\n",
        "    \"\"\"Vectorize data into ngram vectors.\n",
        "\n",
        "    Args:\n",
        "        train_contents: training instances\n",
        "        val_contents: validation instances\n",
        "        y_train: labels of train data.\n",
        "\n",
        "    Returns:\n",
        "        sparse ngram vectors of train, valid text inputs.\n",
        "    \"\"\"\n",
        "    tokenizer = text.Tokenizer(num_words = MAX_VOCAB_SIZE)\n",
        "    tokenizer.fit_on_texts(train_contents)\n",
        "    x_train = tokenizer.texts_to_sequences(train_contents)\n",
        "\n",
        "    if val_contents:\n",
        "        x_val = tokenizer.texts_to_sequences(val_contents)\n",
        "\n",
        "    max_length = len(max(x_train, key=len))\n",
        "    if max_length > MAX_SEQ_LENGTH:\n",
        "        max_length = MAX_SEQ_LENGTH\n",
        "\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "    if val_contents:\n",
        "        x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    num_features = min(len(word_index) + 1, MAX_VOCAB_SIZE)\n",
        "    if val_contents:\n",
        "        return x_train, x_val, word_index, num_features, tokenizer, max_length\n",
        "    else:\n",
        "        return x_train, word_index, num_features, tokenizer, max_length\n",
        "\n",
        "\n",
        "def _get_last_layer_units_and_activation(num_classes):\n",
        "    \"\"\"Gets the # units and activation function for the last network layer.\n",
        "\n",
        "    Args:\n",
        "        num_classes: Number of classes.\n",
        "\n",
        "    Returns:\n",
        "        units, activation values.\n",
        "    \"\"\"\n",
        "    if num_classes == 2:\n",
        "        activation = 'sigmoid'\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = 'softmax'\n",
        "        units = num_classes\n",
        "    return units, activation\n",
        "\n",
        "\n",
        "def sep_cnn_model(input_shape,\n",
        "                  num_classes,\n",
        "                  num_features,\n",
        "                  blocks=1,\n",
        "                  filters=64,\n",
        "                  kernel_size=4,\n",
        "                  dropout_rate=0.25):\n",
        "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(Embedding(input_dim=num_features, output_dim=200, input_length=input_shape))\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(op_units, activation=op_activation))\n",
        "\n",
        "    # for _ in range(blocks - 1):\n",
        "    #     model.add(Dropout(rate=dropout_rate))\n",
        "    #     model.add(SeparableConv1D(filters=filters,\n",
        "    #                               kernel_size=kernel_size,\n",
        "    #                               activation='relu',\n",
        "    #                               bias_initializer='random_uniform',\n",
        "    #                               depthwise_initializer='random_uniform',\n",
        "    #                               padding='same'))\n",
        "    #     model.add(SeparableConv1D(filters=filters,\n",
        "    #                               kernel_size=kernel_size,\n",
        "    #                               activation='relu',\n",
        "    #                               bias_initializer='random_uniform',\n",
        "    #                               depthwise_initializer='random_uniform',\n",
        "    #                               padding='same'))\n",
        "    #     model.add(MaxPooling1D(pool_size=3))\n",
        "\n",
        "    # model.add(SeparableConv1D(filters=filters * 2,\n",
        "    #                           kernel_size=kernel_size,\n",
        "    #                           activation='relu',\n",
        "    #                           bias_initializer='random_uniform',\n",
        "    #                           depthwise_initializer='random_uniform',\n",
        "    #                           padding='same'))\n",
        "    # model.add(SeparableConv1D(filters=filters * 2,\n",
        "    #                           kernel_size=kernel_size,\n",
        "    #                           activation='relu',\n",
        "    #                           bias_initializer='random_uniform',\n",
        "    #                           depthwise_initializer='random_uniform',\n",
        "    #                           padding='same'))\n",
        "\n",
        "    # model.add(GlobalAveragePooling1D())\n",
        "    # # model.add(MaxPooling1D())\n",
        "    # model.add(Dropout(rate=0.5))\n",
        "    # model.add(Dense(op_units, activation=op_activation))\n",
        "    return model\n",
        "\n",
        "\n",
        "def _tokenize_chinese_words(text):\n",
        "    return ' '.join(jieba.cut(text, cut_all=False))\n",
        "\n",
        "def vectorize_data(x_train, x_val=None):\n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "    if x_val:\n",
        "        full_text = x_train + x_val\n",
        "    else:\n",
        "        full_text = x_train\n",
        "    vectorizer.fit(full_text)\n",
        "    train_vectorized = vectorizer.transform(x_train)\n",
        "    if x_val:\n",
        "        val_vectorized = vectorizer.transform(x_val)\n",
        "        return train_vectorized, val_vectorized, vectorizer\n",
        "    return train_vectorized, vectorizer\n",
        "\n",
        "\n",
        "# onhot encode to category\n",
        "def ohe2cat(label):\n",
        "    return np.argmax(label, axis=1)\n",
        "\n",
        "\n",
        "class Model(object):\n",
        "    \"\"\" \n",
        "        model of CNN baseline without pretraining.\n",
        "        see `https://aclweb.org/anthology/D14-1181` for more information.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, metadata, train_output_path=\"./\", test_input_path=\"./\"):\n",
        "        \"\"\" Initialization for model\n",
        "        :param metadata: a dict formed like:\n",
        "            {\"class_num\": 10,\n",
        "             \"language\": ZH,\n",
        "             \"num_train_instances\": 10000,\n",
        "             \"num_test_instances\": 1000,\n",
        "             \"time_budget\": 300}\n",
        "        \"\"\"\n",
        "        self.done_training = False\n",
        "        self.metadata = metadata\n",
        "        self.train_output_path = train_output_path\n",
        "        self.test_input_path = test_input_path\n",
        "\n",
        "    def train(self, train_dataset, remaining_time_budget=None):\n",
        "        \"\"\"model training on train_dataset.\n",
        "        \n",
        "        :param train_dataset: tuple, (x_train, y_train)\n",
        "            x_train: list of str, input training sentences.\n",
        "            y_train: A `numpy.ndarray` matrix of shape (sample_count, class_num).\n",
        "                     here `sample_count` is the number of examples in this dataset as train\n",
        "                     set and `class_num` is the same as the class_num in metadata. The\n",
        "                     values should be binary.\n",
        "        :param remaining_time_budget:\n",
        "        \"\"\"\n",
        "        if self.done_training:\n",
        "            return\n",
        "        x_train, y_train = train_dataset\n",
        "\n",
        "        # tokenize Chinese words\n",
        "        if self.metadata['language'] == 'ZH':\n",
        "            x_train = clean_zh_text(x_train)\n",
        "            x_train = list(map(_tokenize_chinese_words, x_train))\n",
        "        else:\n",
        "            x_train = clean_en_text(x_train)\n",
        "\n",
        "        x_train, word_index, num_features, tokenizer, max_length = sequentialize_data(x_train)\n",
        "        num_classes = self.metadata['class_num']\n",
        "\n",
        "        # initialize model\n",
        "        model = sep_cnn_model(input_shape=x_train.shape[1:][0],\n",
        "                              num_classes=num_classes,\n",
        "                              num_features=num_features,\n",
        "                              blocks=2,\n",
        "                              filters=64,\n",
        "                              kernel_size=4,\n",
        "                              dropout_rate=0.25)\n",
        "        if num_classes == 2:\n",
        "            loss = 'binary_crossentropy'\n",
        "        else:\n",
        "            loss = 'sparse_categorical_crossentropy'\n",
        "        optimizer = tf.keras.optimizers.Adam(lr=0.0005)\n",
        "        model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "        callbacks = [tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=10)]\n",
        "        history = model.fit(\n",
        "            x_train,\n",
        "            ohe2cat(y_train),\n",
        "            epochs=100,\n",
        "            callbacks=callbacks,\n",
        "            validation_split=0.1,\n",
        "            verbose=2,  # Logs once per epoch.\n",
        "            batch_size=32,\n",
        "            shuffle=True)\n",
        "        print(str(type(x_train)) + \" \" + str(y_train.shape))\n",
        "        model.save(self.train_output_path + 'model.h5')\n",
        "        with open(self.train_output_path + 'tokenizer.pickle', 'wb') as handle:\n",
        "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        with open(self.train_output_path + 'model.config', 'wb') as f:\n",
        "            f.write(str(max_length).encode())\n",
        "            f.close()\n",
        "\n",
        "        self.done_training=True\n",
        "\n",
        "    def test(self, x_test, remaining_time_budget=None):\n",
        "        \"\"\"\n",
        "        :param x_test: list of str, input test sentences.\n",
        "        :param remaining_time_budget:\n",
        "        :return: A `numpy.ndarray` matrix of shape (sample_count, class_num).\n",
        "                 here `sample_count` is the number of examples in this dataset as test\n",
        "                 set and `class_num` is the same as the class_num in metadata. The\n",
        "                 values should be binary or in the interval [0,1].\n",
        "        \"\"\"\n",
        "        model = models.load_model(self.test_input_path + 'model.h5')\n",
        "        with open(self.test_input_path + 'tokenizer.pickle', 'rb') as handle:\n",
        "            tokenizer = pickle.load(handle, encoding='iso-8859-1')\n",
        "        with open(self.test_input_path + 'model.config', 'r') as f:\n",
        "            max_length = int(f.read().strip())\n",
        "            f.close()\n",
        "\n",
        "        train_num, test_num = self.metadata['train_num'], self.metadata['test_num']\n",
        "        class_num = self.metadata['class_num']\n",
        "\n",
        "        # tokenizing Chinese words\n",
        "        if self.metadata['language'] == 'ZH':\n",
        "            x_test = clean_zh_text(x_test)\n",
        "            x_test = list(map(_tokenize_chinese_words, x_test))\n",
        "        else:\n",
        "            x_test = clean_en_text(x_test)\n",
        "\n",
        "        x_test = tokenizer.texts_to_sequences(x_test)\n",
        "        x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n",
        "        result = model.predict_classes(x_test)\n",
        "\n",
        "        # category class list to sparse class list of lists\n",
        "        y_test = np.zeros([test_num, class_num])\n",
        "        for idx, y in enumerate(result):\n",
        "            y_test[idx][y] = 1\n",
        "        return y_test\n",
        "\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW7lNA-73QZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path = '/content/sample_data'\n",
        "model = Model(autoDaset.get_metadata(),model_path,model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjA4ajAY4ZI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data, y_data = autoDaset.get_train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKj-2z4J4iY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, Y_train = x_data[:6000], y_data[:6000]\n",
        "X_val, Y_val = x_data[6000:],y_data[6000:]\n",
        "\n",
        "train_data = (X_train, Y_train)\n",
        "val_data = (X_val, Y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESOJCxHs4n2C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "c596e076-bc38-4db7-b9d1-5cd51ee082d3"
      },
      "source": [
        "model.train(train_data)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5400 samples, validate on 600 samples\n",
            "Epoch 1/100\n",
            "5400/5400 - 25s - loss: 0.6658 - acc: 0.5857 - val_loss: 0.5993 - val_acc: 0.6867\n",
            "Epoch 2/100\n",
            "5400/5400 - 24s - loss: 0.4229 - acc: 0.8189 - val_loss: 0.4134 - val_acc: 0.8217\n",
            "Epoch 3/100\n",
            "5400/5400 - 24s - loss: 0.2111 - acc: 0.9187 - val_loss: 0.4099 - val_acc: 0.8217\n",
            "Epoch 4/100\n",
            "5400/5400 - 24s - loss: 0.1094 - acc: 0.9622 - val_loss: 0.5791 - val_acc: 0.8117\n",
            "Epoch 5/100\n",
            "5400/5400 - 24s - loss: 0.0615 - acc: 0.9806 - val_loss: 0.6654 - val_acc: 0.8117\n",
            "Epoch 6/100\n",
            "5400/5400 - 24s - loss: 0.0418 - acc: 0.9883 - val_loss: 0.5933 - val_acc: 0.8050\n",
            "Epoch 7/100\n",
            "5400/5400 - 24s - loss: 0.0284 - acc: 0.9924 - val_loss: 0.6252 - val_acc: 0.8167\n",
            "Epoch 8/100\n",
            "5400/5400 - 24s - loss: 0.0218 - acc: 0.9941 - val_loss: 0.7355 - val_acc: 0.8150\n",
            "Epoch 9/100\n",
            "5400/5400 - 24s - loss: 0.0141 - acc: 0.9961 - val_loss: 0.7643 - val_acc: 0.8100\n",
            "Epoch 10/100\n",
            "5400/5400 - 24s - loss: 0.0095 - acc: 0.9974 - val_loss: 0.8177 - val_acc: 0.8150\n",
            "Epoch 11/100\n",
            "5400/5400 - 24s - loss: 0.0056 - acc: 0.9985 - val_loss: 0.8873 - val_acc: 0.8117\n",
            "Epoch 12/100\n",
            "5400/5400 - 24s - loss: 0.0071 - acc: 0.9987 - val_loss: 0.7845 - val_acc: 0.8083\n",
            "Epoch 13/100\n",
            "5400/5400 - 24s - loss: 0.0075 - acc: 0.9983 - val_loss: 0.9496 - val_acc: 0.7917\n",
            "<class 'numpy.ndarray'> (6000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7DVzdyH5a9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = model.test(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55fEH7lt7zHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4c8ad09c-fc47-422a-8a40-5983f87c4145"
      },
      "source": [
        "Y_val"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1.],\n",
              "       [0., 1.],\n",
              "       [0., 1.],\n",
              "       ...,\n",
              "       [1., 0.],\n",
              "       [0., 1.],\n",
              "       [0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY-J6Td57_iM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predict = ohe2cat(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ycv6JM78CTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = ohe2cat(Y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNjQKKYQ8UoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "943ac188-e09f-453c-9f0f-29462e3153ab"
      },
      "source": [
        "predict_good = np.equal(y_predict[:1792],y_true)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7633928571428571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}