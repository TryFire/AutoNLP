{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "analyse.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP6ttzFtyS2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpwV5RaN0_IP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "210910fd-f329-4364-855e-d43f691fe055"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRWY_jhyBjmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2016 Google Inc. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"\n",
        "  AutoNLP datasets.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class AutoNLPDataset(object):\n",
        "    def __init__(self, dataset_dir):\n",
        "        \"\"\"\n",
        "            train_dataset, test_dataset: list of strings\n",
        "            train_label: np.array\n",
        "        \"\"\"\n",
        "        self.dataset_name_ = dataset_dir\n",
        "        self.dataset_dir_ = dataset_dir\n",
        "        self.metadata_ = self.read_metadata(os.path.join(dataset_dir, \"meta.json\"))\n",
        "\n",
        "    def read_dataset(self):\n",
        "        self.train_dataset = self._read_dataset(os.path.join(self.dataset_dir_, \"train.data\"))\n",
        "        self.train_label = self.read_label(os.path.join(self.dataset_dir_, \"train.solution\"))\n",
        "        self.test_dataset = self._read_dataset(os.path.join(self.dataset_dir_, \"test.data\"))\n",
        "\n",
        "    def get_train(self):\n",
        "        return self.train_dataset, self.train_label\n",
        "\n",
        "    def get_test(self):\n",
        "        return self.test_dataset\n",
        "\n",
        "    def get_metadata(self):\n",
        "        return self.metadata_\n",
        "\n",
        "    def read_metadata(self, metadata_path):\n",
        "        import json\n",
        "        return json.load(open(metadata_path))\n",
        "\n",
        "    def _read_dataset(self, dataset_path):\n",
        "        with open(dataset_path) as fin:\n",
        "            return fin.readlines()\n",
        "\n",
        "    def read_label(self, label_path):\n",
        "        return np.loadtxt(label_path)\n",
        "\n",
        "    def get_class_num(self):\n",
        "        \"\"\" return the number of class \"\"\"\n",
        "        return self.metadata_[\"class_num\"]\n",
        "\n",
        "    def get_train_num(self):\n",
        "        \"\"\" return the number of train instance \"\"\"\n",
        "        return self.metadata_[\"train_num\"]\n",
        "\n",
        "    def get_test_num(self):\n",
        "        \"\"\" return the number of test instance \"\"\"\n",
        "        return self.metadata_[\"test_num\"]\n",
        "\n",
        "    def get_language(self):\n",
        "        \"\"\" ZH or EN \"\"\"\n",
        "        return self.metadata_[\"language\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAXTeWQ4BVPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.autograd as ag\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import jieba\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.autograd as autograd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkF-CrvWyS3Q",
        "colab_type": "text"
      },
      "source": [
        "### Fun Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfhWuSnLyS3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_en_text(dat):\n",
        "\n",
        "    REPLACE_BY_SPACE_RE = re.compile('[\"/(){}\\[\\]\\|@,;]')\n",
        "    BAD_SYMBOLS_RE = re.compile('[^0-9a-zA-Z #+_]')\n",
        "\n",
        "    ret = []\n",
        "    for line in dat:\n",
        "        # text = text.lower() # lowercase text\n",
        "        line = REPLACE_BY_SPACE_RE.sub(' ', line)\n",
        "        line = BAD_SYMBOLS_RE.sub('', line)\n",
        "        line = line.strip()\n",
        "        ret.append(line)\n",
        "    return ret\n",
        "\n",
        "def clean_zh_text(dat):\n",
        "    REPLACE_BY_SPACE_RE = re.compile('[“”【】/（）：！～「」、|，；。\"/(){}\\[\\]\\|@,\\.;]')\n",
        "\n",
        "    ret = []\n",
        "    for line in dat:\n",
        "        line = REPLACE_BY_SPACE_RE.sub(' ', line)\n",
        "        line = line.strip()\n",
        "        ret.append(line)\n",
        "    return ret\n",
        "\n",
        "\n",
        "def _tokenize_chinese_words(text):\n",
        "    return ' '.join(jieba.cut(text, cut_all=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP9mVoG9yS3h",
        "colab_type": "text"
      },
      "source": [
        "## Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xh22Wi0yS3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = '/content/drive/My Drive/autonlp_starting_kit-master/offline_data/O1/O1.data'\n",
        "autoDaset = AutoNLPDataset(dataset_path)\n",
        "autoDaset.read_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mgCdOF-yS3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data, y_data = autoDaset.get_train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m9v5Yb7yS4B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "d060193a-69b6-45f4-8206-1265b7ae29b9"
      },
      "source": [
        "# take a look at the what the data look like\n",
        "print(x_data[:5])\n",
        "print(y_data[:5])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a stirring , funny and finally transporting re-imagining of beauty and the beast and 1930s horror films \\n', 'apparently reassembled from the cutting-room floor of any given daytime soap . \\n', \"they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science-fiction elements of bug-eyed monsters and futuristic women in skimpy clothes . \\n\", 'this is a visually stunning rumination on love , memory , history and the war between art and commerce . \\n', \"jonathan parker 's bartleby should have been the be-all-end-all of the modern-office anomie films . \\n\"]\n",
            "[[0. 1.]\n",
            " [1. 0.]\n",
            " [1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H97Lcpr7yS4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac93712d-96df-4af9-f8e9-ae158e05a6c3"
      },
      "source": [
        "num_class = autoDaset.get_metadata()['class_num']\n",
        "print('number of classes : ', num_class)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of classes :  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxsGGqS7Txb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syly4nwMyS4g",
        "colab_type": "text"
      },
      "source": [
        "## Data pre-processing\n",
        "\n",
        "- The data has many punctuations, we can replace it by space.\n",
        "\n",
        "- There is not valid set, so we will split the whole dataset into transet ans valid set to better generate the training and hyper-parameters.\n",
        "\n",
        "- Each observation is a sentence which is hard to translated into presentation, we will split it to tokens.\n",
        "\n",
        "- We will convert data to Pytorch tensors so they can be used in a neural network. To do that, you must first create a dictionnary that will map words to integers. Add to the dictionnary only words that are in the training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQD-auXLyS4j",
        "colab_type": "text"
      },
      "source": [
        "clean sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOC2Fk4IyS4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if autoDaset.get_metadata()['language'] == 'ZH':\n",
        "    x_data = clean_zh_text(x_data)\n",
        "    x_data = list(map(_tokenize_chinese_words, x_data))\n",
        "else:\n",
        "    x_data = clean_en_text(x_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx6m8AoRyS4t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4539a992-75a2-47e5-cb03-bd38790b2e0a"
      },
      "source": [
        "print('number of transet : ', len(x_data))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of transet :  7792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mExGN4CcyS4z",
        "colab_type": "text"
      },
      "source": [
        "tokenize dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2gM4d4WyS41",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "a41b34b6-38ca-41e5-e5d7-fe6130e844ae"
      },
      "source": [
        "x_data = list(map(word_tokenize, x_data))\n",
        "x_data[0]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'stirring',\n",
              " 'funny',\n",
              " 'and',\n",
              " 'finally',\n",
              " 'transporting',\n",
              " 'reimagining',\n",
              " 'of',\n",
              " 'beauty',\n",
              " 'and',\n",
              " 'the',\n",
              " 'beast',\n",
              " 'and',\n",
              " '1930s',\n",
              " 'horror',\n",
              " 'films']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZktcU0vMWo07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "156af692-60b6-438a-ce52-e433350aeb1b"
      },
      "source": [
        "long_n = np.mean(np.array(list(map(len, x_data)))>15)\n",
        "ticklabels = ['long sentences','short sentences']\n",
        "plt.bar(range(2), [long_n,1-long_n],color='rgb',tick_label=class_name)\n",
        "plt.title('long sentence ratio')\n",
        "plt.show()\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAP3klEQVR4nO3dfZBdd13H8fenCSkPDY9ZKSRpU6CC\nQSrokjLOCEWKpqgNM6CmM0qLZSKOQRz4g450ClT4oyAPauNIBkoRBkLtyBhKtCBSpKNlsgUGTGuG\nJbYmgdKllCcRSuDrH/cEbrd3d0+am2z3l/dr5s7c38M953t2N589+zv3nqSqkCQtfSctdgGSpPEw\n0CWpEQa6JDXCQJekRhjoktQIA12SGmGg64gkuS3JuYtdh+aX5LtJnrDYdej4MtB1wkpyQ5KXLXYd\nR2vUcVTVKVW1b7Fq0uIw0KUHsCTLF7sGLR0Guu63JCcneUeSr3SPdyQ5uRs7J8mBJK9OcmeSryZ5\n6dBrH5PkI0m+nWR3kjcmuXGO/Tw4yfuT3JXkm938x3Zjj0jy7m77B7vtLOvGLkpyY5K/SHJ3kv9O\ncl439ibgV4Aru+WJK7v+pyT5eJJvJNmb5HeG6rg6ybYkH03ynSSfSfLEofGnDr32a0n+rOs/Kckl\nSb7cHcM1SR49x7Ee/rq9JskdwHuSPCrJdUlmuuO4LsmaBY6jkjxp6Gv0d93rb09yaRL/7TfIb6qO\nxmuBZwFPB34B2ABcOjR+KvAIYDVwMbAtyaO6sW3A/3ZzLuwec7mw285a4DHAy4H/68auBg4BTwKe\nAfwaMLz8cDawF1gFvBl4d5JU1WuBTwNbu+WJrUkeBnwc+ADwM8Bm4G+SrB/a3mbgDcCjgGngTQBJ\nVgL/Avwz8Piunk90r3kF8ELgOd3Y3d3xz+VU4NHA6cAWBv9O39O1T+uO/UqAUccxYnt/3X39ntDV\n8BLgpSPmaamrKh8+ej+A24Bzu+dfBl4wNPbrwG3d83MYBM/yofE7GfwCWAb8EHjy0NgbgRvn2Ocf\nAP8OnDWr/7HAD4CHDPVdAHyye34RMD009lCggFO79g3Ay4bGfxf49Kx9vBN4Xff8auBdQ2MvAP5r\naL+fm6P+W4HnDbUf1x3/8hFzzwHuAR48z/fg6cDdQ+17HUfXVwx+qSzrtrd+aOwPgRsW+2fJx/gf\nrs/paDweuH2ofXvXd9hdVXVoqP094BRgAlgO7B8aG34+2/sYnJ3vSPJI4P0M/jo4HXgQ8NUkh+ee\nNGtbdxx+UlXf6+adMsd+TgfOTvLNob7l3f7vs72h46Gr78vzbPfDSX481PcjBr+QDo6YP1NV3z/c\nSPJQ4O3ARgZ/GQCsTLKsqn40xz4PW8XgazT7+7R6gddpCXLJRUfjKwzC6rDTur6FzDBYJlkz1Ld2\nrslV9cOqekNVrQd+GfhNBssG+xmcoa+qqkd2j4dX1VN71j/7VqP7gU8NbeuRNVjG+KMe29rPYElj\nrrHzZm33wVU1KsxH1fVq4MnA2VX1cODZXX/mmD/s6wz+Gpj9fZpr31rCDHQdjQ8ClyaZSLIKuIzB\n2fO8urPKfwBen+ShSZ7CIKBHSvLcJE/rLnZ+m0FA/biqvgp8DHhrkod3Fx+fmOQ5Pev/GvcO4euA\nn03y+0ke1D2emeTnemzrOuBxSf60u1i8MsnZ3djfAm9Kcnp3PBNJNvWsEWAlg+Wrb3YXU1+3wHH8\nRPe1vqbb/8quhlfR4/ukpcdA19F4IzAFfAH4IvDZrq+PrQwu1N3BYEnjgwzOtkc5FbiWQZjfCnyK\nny6DvARYAdzC4GLjtQzWqPv4S+DF3TtH/qqqvsPgoupmBn9p3AFcAZy80Ia61z4f+K3udV8Cnju0\nn53Ax5J8B7iJwcXavt4BPITB2fZNDC68znkcI17/CgYXoPcBNzK46HvVEexfS0Sq/A8utPiSXMHg\nYuV873aRNA/P0LUouvd7n5WBDQze1vjhxa5LWsp8l4sWy0oGyyyPZ7AG/FbgHxe1ImmJc8lFkhrh\nkoskNWLRllxWrVpV69atW6zdS9KSdPPNN3+9qiZGjS1aoK9bt46pqanF2r0kLUlJbp9rzCUXSWqE\ngS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqxNK82+JP//9I6b684ZxOUJ6hS1Ij\nDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNaJX\noCfZmGRvkukkl4wYvyjJTJLPd4+Xjb9USdJ8Frx9bpJlwDbg+cABYHeSnVV1y6ypH6qqrcegRklS\nD33O0DcA01W1r6ruAXYAm45tWZKkI9Un0FcD+4faB7q+2V6U5AtJrk2ydtSGkmxJMpVkamZm5n6U\nK0may7guin4EWFdVZwEfB947alJVba+qyaqanJiYGNOuJUnQL9APAsNn3Gu6vp+oqruq6gdd813A\nL42nPElSX30CfTdwZpIzkqwANgM7hyckedxQ83zg1vGVKEnqY8F3uVTVoSRbgeuBZcBVVbUnyeXA\nVFXtBP4kyfnAIeAbwEXHsGZJ0gipRfof0icnJ2tqaur+vTgZbzFqyyL9TEvHQ5Kbq2py1JifFJWk\nRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhqx4M25JB25vMH7\nDWlu9bpjc78hz9AlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJA\nl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEb0CPcnGJHuTTCe5ZJ55L0pSSSbHV6IkqY8F\nAz3JMmAbcB6wHrggyfoR81YCrwQ+M+4iJUkL63OGvgGYrqp9VXUPsAPYNGLenwNXAN8fY32SpJ76\nBPpqYP9Q+0DX9xNJfhFYW1UfnW9DSbYkmUoyNTMzc8TFSpLmdtQXRZOcBLwNePVCc6tqe1VNVtXk\nxMTE0e5akjSkT6AfBNYOtdd0fYetBH4euCHJbcCzgJ1eGJWk46tPoO8GzkxyRpIVwGZg5+HBqvpW\nVa2qqnVVtQ64CTi/qqaOScWSpJEWDPSqOgRsBa4HbgWuqao9SS5Pcv6xLlCS1M/yPpOqahewa1bf\nZXPMPefoy5IkHSk/KSpJjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWp\nEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhph\noEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1IhegZ5kY5K9SaaTXDJi/OVJvpjk80luTLJ+\n/KVKkuazYKAnWQZsA84D1gMXjAjsD1TV06rq6cCbgbeNvVJJ0rz6nKFvAKaral9V3QPsADYNT6iq\nbw81HwbU+EqUJPWxvMec1cD+ofYB4OzZk5L8MfAqYAXwq6M2lGQLsAXgtNNOO9JaJUnzGNtF0ara\nVlVPBF4DXDrHnO1VNVlVkxMTE+PatSSJfoF+EFg71F7T9c1lB/DCoylKknTk+gT6buDMJGckWQFs\nBnYOT0hy5lDzN4Avja9ESVIfC66hV9WhJFuB64FlwFVVtSfJ5cBUVe0EtiY5F/ghcDdw4bEsWpJ0\nX30uilJVu4Bds/ouG3r+yjHXJUk6Qn5SVJIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjo\nktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5J\njTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEb0CPcnGJHuTTCe5ZMT4\nq5LckuQLST6R5PTxlypJms+CgZ5kGbANOA9YD1yQZP2saZ8DJqvqLOBa4M3jLlSSNL8+Z+gbgOmq\n2ldV9wA7gE3DE6rqk1X1va55E7BmvGVKkhbSJ9BXA/uH2ge6vrlcDPzTqIEkW5JMJZmamZnpX6Uk\naUFjvSia5PeASeAto8arantVTVbV5MTExDh3LUknvOU95hwE1g6113R995LkXOC1wHOq6gfjKU+S\n1FefM/TdwJlJzkiyAtgM7ByekOQZwDuB86vqzvGXKUlayIKBXlWHgK3A9cCtwDVVtSfJ5UnO76a9\nBTgF+Pskn0+yc47NSZKOkT5LLlTVLmDXrL7Lhp6fO+a6JElHyE+KSlIjDHRJaoSBLkmNMNAlqREG\nuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBL\nUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1\nolegJ9mYZG+S6SSXjBh/dpLPJjmU5MXjL1OStJAFAz3JMmAbcB6wHrggyfpZ0/4HuAj4wLgLlCT1\ns7zHnA3AdFXtA0iyA9gE3HJ4QlXd1o39+BjUKEnqoc+Sy2pg/1D7QNd3xJJsSTKVZGpmZub+bEKS\nNIfjelG0qrZX1WRVTU5MTBzPXUtS8/oE+kFg7VB7TdcnSXoA6RPou4Ezk5yRZAWwGdh5bMuSJB2p\nBQO9qg4BW4HrgVuBa6pqT5LLk5wPkOSZSQ4Avw28M8meY1m0JOm++rzLharaBeya1XfZ0PPdDJZi\nJEmLxE+KSlIjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQ\nJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12S\nGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1olegJ9mYZG+S6SSXjBg/OcmHuvHPJFk37kIlSfNbMNCT\nLAO2AecB64ELkqyfNe1i4O6qehLwduCKcRcqSZpfnzP0DcB0Ve2rqnuAHcCmWXM2Ae/tnl8LPC9J\nxlemJGkhy3vMWQ3sH2ofAM6ea05VHUryLeAxwNeHJyXZAmzpmt9Nsvf+FK37WMWsr/UJzXOJByJ/\nRofk9Uf1M3r6XAN9An1sqmo7sP147vNEkGSqqiYXuw5pLv6MHh99llwOAmuH2mu6vpFzkiwHHgHc\nNY4CJUn99An03cCZSc5IsgLYDOycNWcncGH3/MXAv1ZVja9MSdJCFlxy6dbEtwLXA8uAq6pqT5LL\ngamq2gm8G3hfkmngGwxCX8ePy1h6oPNn9DiIJ9KS1AY/KSpJjTDQJakRBvoSttAtGaTFluSqJHcm\n+c/FruVEYKAvUT1vySAttquBjYtdxInCQF+6+tySQVpUVfVvDN75puPAQF+6Rt2SYfUi1SLpAcBA\nl6RGGOhLV59bMkg6gRjoS1efWzJIOoEY6EtUVR0CDt+S4Vbgmqras7hVSfeW5IPAfwBPTnIgycWL\nXVPL/Oi/JDXCM3RJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhrx/+5iRd2XzqDEAAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwaJmHFIyS49",
        "colab_type": "text"
      },
      "source": [
        "Split into train set and valid set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0rnbbWNyS4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split data into transet and valset\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--T92gZkyS5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6aebe91e-59bb-4489-aa0a-dd401318229e"
      },
      "source": [
        "print('N X_train : ', len(X_train))\n",
        "print('N X_val : ', len(X_val))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N X_train :  6233\n",
            "N X_val :  1559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hni2ij2kyS5N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# onhot encode to category\n",
        "def ohe2cat(label):\n",
        "    return np.argmax(label, axis=1)\n",
        "Y_train, Y_val = ohe2cat(Y_train), ohe2cat(Y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsew8m7JyS5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "f6898dbe-689a-4b4e-fe6a-a962d368eb89"
      },
      "source": [
        "## compute number of occurence of each class\n",
        "num_class = autoDaset.get_class_num()\n",
        "\n",
        "class_name = [i for i in range(num_class)]\n",
        "class_n = [np.sum(Y_train==class_name[i])  for i in range(num_class)]\n",
        "\n",
        "plt.bar(range(num_class), class_n,color='rgb',tick_label=class_name)\n",
        "plt.title('Number of observations of each class')\n",
        "plt.show()\n",
        "\n",
        "## it is balanced, good"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwIb89ozKIP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10e239d5-ec1e-4bd8-b18f-f0eab43b308c"
      },
      "source": [
        "sentences_length = np.array(list(map(len,X_train)))\n",
        "np.mean(sentences_length > 15)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5456441520936949"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv00dciuyS5c",
        "colab_type": "text"
      },
      "source": [
        "Create a dictionnary that will map words to integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6JhP6WjyS5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = {}\n",
        "num_voc = 0\n",
        "for sentence in X_train:\n",
        "    for word in sentence:\n",
        "        if word not in dictionary.keys():\n",
        "            dictionary[word] = num_voc\n",
        "            num_voc += 1\n",
        "dictionary['UNK'] = num_voc\n",
        "num_voc+=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm3psBnyyS5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_tensor(data):\n",
        "    d = []\n",
        "    for word in data:\n",
        "        if word in dictionary.keys():\n",
        "            d.append(dictionary[word])\n",
        "    return th.tensor(d)\n",
        "\n",
        "def to_index(data):\n",
        "    d = []\n",
        "    for word in data:\n",
        "        if word in dictionary.keys():\n",
        "            d.append(dictionary[word])\n",
        "        else:\n",
        "            d.append(dictionary['UNK'])\n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJrARhd4yS5p",
        "colab_type": "text"
      },
      "source": [
        "## The model\n",
        "\n",
        "- The dataset can not be encoded by one-hot, but embedding is a good choice. So it takes as input a tensor that is a sequence of integers indexing word embeddings.\n",
        "\n",
        "- a"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF5WWTdmyS5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class My_classifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, class_num):\n",
        "        super(My_classifier, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 32)\n",
        "        if class_num == 2:\n",
        "          self.linear2 = nn.Linear(32, 1)\n",
        "        else:\n",
        "          self.linear2 = nn.Linear(32, class_num)\n",
        "        \n",
        "        nn.init.kaiming_uniform_(self.linear1.weight.data)\n",
        "        nn.init.kaiming_uniform_(self.linear2.weight.data)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).sum(0)\n",
        "        out = th.relu(self.linear1(embeds.view(1,-1)))\n",
        "        out = self.linear2(out)\n",
        "        #log_probs = th.log_softmax(out, dim=1)\n",
        "        return out\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azgqb3FoyS5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZMeBQNzyS54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate(model, datas, labels):\n",
        "    '''\n",
        "    function for evaluation\n",
        "    model : pytorch model\n",
        "    datas : dataset \n",
        "    labels : labels\n",
        "\n",
        "    return : accuracy\n",
        "    '''\n",
        "    # nb of good prediction\n",
        "    good = 0\n",
        "    # nb of false prediction\n",
        "    total = 0\n",
        "\n",
        "    # for each paire of x,y \n",
        "    for x,y in zip(datas,labels):\n",
        "        # make x to Tensor\n",
        "        x = to_tensor(x)\n",
        "        # if number of words in a sentence is smaller than size of slide window\n",
        "        total += 1\n",
        "        if x.size()[0] <= 0:\n",
        "            continue\n",
        "        # we break\n",
        "        # get result of model\n",
        "        out = model(x)\n",
        "    \n",
        "        if num_class == 2:\n",
        "            if (out[0].item() >= 0) == (y == 1.):\n",
        "                good += 1\n",
        "        else:\n",
        "        \n",
        "            if np.argmax(out.detach().numpy()) == y:\n",
        "                good += 1\n",
        "    return good/total\n",
        "\n",
        "def train(model, optimizer, criterion, data, labels):\n",
        "    total_loss = 0\n",
        "    data, labels = shuffle(data, labels)\n",
        "    for x, y in zip(data, labels):\n",
        "        # reset the gradient\n",
        "        optimizer.zero_grad()\n",
        "#         if to_tensor(x).size()[0] <= WINDOW-1:\n",
        "#             continue\n",
        "        # output of the model\n",
        "        output = model(to_tensor(x))\n",
        "        # compute the loss\n",
        "        loss = criterion(output, th.tensor([y]))\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()  # compute gradient\n",
        "        #torch.nn.utils.clip_grad_value_(model.parameters(), 5.)  # clip gradient if its norm exceed 5\n",
        "        optimizer.step()  # update parameters\n",
        "    \n",
        "    return model,total_loss/len(data)\n",
        "\n",
        "def training_loop(model, optimizer, loss_function, n_epochs, datas, labels):\n",
        "    '''\n",
        "    training function\n",
        "    \n",
        "    model : the torch model\n",
        "    optimizer : optimizer \n",
        "    loss_function : loss function\n",
        "    n_epochs : number of epches\n",
        "    datas : dataset (x)\n",
        "    labels : labels (y)\n",
        "\n",
        "    return : the trained model, list of mean loss of each epochs, list of accuracy on dev set of each epochs, \n",
        "    '''\n",
        "    mean_losss = []\n",
        "    dev_accus = []\n",
        "    for e in range(n_epochs):\n",
        "        print(\"Epoch : \",e)\n",
        "        # train the model\n",
        "        model,mean_loss = train(model, optimizer, loss_function, datas, labels)\n",
        "        \n",
        "        #dev_accus.append(accuracy)  \n",
        "        mean_losss.append(mean_loss)\n",
        "\n",
        "        print('mean loss : ', mean_loss)\n",
        "        print('accuracy on dev set : ', evaluate(model, X_val, Y_val))\n",
        "    return model, mean_losss\n",
        "\n",
        "def training_loop_batch(model, optimizer, loss_function, n_epochs, datas, labels):\n",
        "    '''\n",
        "    training function\n",
        "    \n",
        "    model : the torch model\n",
        "    optimizer : optimizer \n",
        "    loss_function : loss function\n",
        "    n_epochs : number of epches\n",
        "    datas : dataset (x)\n",
        "    labels : labels (y)\n",
        "\n",
        "    return : the trained model, list of mean loss of each epochs, list of accuracy on dev set of each epochs, \n",
        "    '''\n",
        "    mean_losss = []\n",
        "    dev_accus = []\n",
        "    for e in range(n_epochs):\n",
        "        print(\"Epoch : \",e)\n",
        "        # train the model\n",
        "        model,mean_loss = train(model, optimizer, loss_function, datas, labels)\n",
        "        \n",
        "        #dev_accus.append(accuracy)  \n",
        "        mean_losss.append(mean_loss)\n",
        "\n",
        "        print('mean loss : ', mean_loss)\n",
        "        print('accuracy on dev set : ', evaluate(model, X_val, Y_val))\n",
        "    return model, mean_losss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuzU_hCwyS6D",
        "colab_type": "text"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAG5WWoYyS6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "IIYjT-0byS6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 20\n",
        "BATCH_SIZE = 10\n",
        "WEIGHT_DECAY = 0.02\n",
        "DROP_OUT = None\n",
        "EMBED_DIM = 40\n",
        "\n",
        "model = My_classifier(num_voc, embedding_dim=EMBED_DIM, class_num=num_class)\n",
        "#model = CBOW_classifier(num_voc, embedding_dim=EMBED_DIM,)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = th.optim.SGD(model.parameters(),lr=0.01)\n",
        "# training loop\n",
        "model, mean_losss = training_loop(model, optimizer, criterion, EPOCHS, X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2Z5p8bMyS64",
        "colab_type": "text"
      },
      "source": [
        "## class LSTM_classifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LSTM_classifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
        "        self.linear = nn.Linear(2*hidden_dim, 50)\n",
        "        self.linear2 = nn.Linear(50,1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        out = self.embedding(inputs)\n",
        "        out, (h0,h1) = self.lstm(out.view(len(inputs), 1, -1))\n",
        "        out = F.relu(self.linear(self.dropout(h0.view(-1))))\n",
        "        out = self.linear2(out)\n",
        "        return out\n",
        "    \n",
        "\n",
        "model = LSTM_classifier(num_voc, 100, 100)\n",
        "model.train()\n",
        "loss = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.5)\n",
        "model, mean_losss = training_loop(model, optimizer, criterion, EPOCHS, X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi62ZsXXyS66",
        "colab_type": "code",
        "colab": {},
        "outputId": "1cca623d-0bf1-4ca5-fed6-a40d120327ad"
      },
      "source": [
        "model(to_tensor(X_train[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0003], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emy_4BDxyS7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model = LSTM_classifier(num_voc,100,100)\n",
        "#model = CBOW_classifier(num_voc, embedding_dim=EMBED_DIM,)\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "print(model)\n",
        "optimizer = th.optim.SGD(model.parameters(),lr=0.01)\n",
        "# training loop\n",
        "model, mean_losss = training_loop(model, optimizer, criterion, EPOCHS, X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T6M3HpXUugL2",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, num_output, rnn_model='LSTM', use_last=True, embedding_tensor=None,\n",
        "                 padding_index=0, hidden_size=64, num_layers=1, batch_first=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: vocab size\n",
        "            embed_size: embedding size\n",
        "            num_output: number of output (classes)\n",
        "            rnn_model:  LSTM or GRU\n",
        "            use_last:  bool\n",
        "            embedding_tensor:\n",
        "            padding_index:\n",
        "            hidden_size: hidden size of rnn module\n",
        "            num_layers:  number of layers in rnn module\n",
        "            batch_first: batch first option\n",
        "        \"\"\"\n",
        "\n",
        "        super(RNN, self).__init__()\n",
        "        self.use_last = use_last\n",
        "        # embedding\n",
        "        self.encoder = None\n",
        "        if torch.is_tensor(embedding_tensor):\n",
        "            self.encoder = nn.Embedding(vocab_size, embed_size, padding_idx=padding_index, _weight=embedding_tensor)\n",
        "            self.encoder.weight.requires_grad = False\n",
        "        else:\n",
        "            self.encoder = nn.Embedding(vocab_size, embed_size, padding_idx=padding_index)\n",
        "\n",
        "        self.drop_en = nn.Dropout(p=0.6)\n",
        "\n",
        "        # rnn module\n",
        "        if rnn_model == 'LSTM':\n",
        "            self.rnn = nn.LSTM( input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, dropout=0.5,\n",
        "                                batch_first=True, bidirectional=True)\n",
        "        elif rnn_model == 'GRU':\n",
        "            self.rnn = nn.GRU( input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, dropout=0.5,\n",
        "                                batch_first=True, bidirectional=True)\n",
        "        else:\n",
        "            raise LookupError(' only support LSTM and GRU')\n",
        "\n",
        "\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_size*2)\n",
        "        self.fc = nn.Linear(hidden_size*2, 1)\n",
        "\n",
        "    def forward(self, x, seq_lengths):\n",
        "        '''\n",
        "        Args:\n",
        "            x: (batch, time_step, input_size)\n",
        "        Returns:\n",
        "            num_output size\n",
        "        '''\n",
        "\n",
        "        x_embed = self.encoder(x)\n",
        "        x_embed = self.drop_en(x_embed)\n",
        "        packed_input = pack_padded_sequence(x_embed, seq_lengths.cpu().numpy(),batch_first=True)\n",
        "\n",
        "        # r_out shape (batch, time_step, output_size)\n",
        "        # None is for initial hidden state\n",
        "        packed_output, ht = self.rnn(packed_input, None)\n",
        "        out_rnn, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        row_indices = torch.arange(0, x.size(0)).long()\n",
        "        col_indices = seq_lengths - 1\n",
        "        if next(self.parameters()).is_cuda:\n",
        "            row_indices = row_indices.cuda()\n",
        "            col_indices = col_indices.cuda()\n",
        "\n",
        "        if self.use_last:\n",
        "            last_tensor=out_rnn[row_indices, col_indices, :]\n",
        "        else:\n",
        "            # use mean\n",
        "            last_tensor = out_rnn[row_indices, :, :]\n",
        "            last_tensor = torch.mean(last_tensor, dim=1)\n",
        "\n",
        "        fc_input = self.bn2(last_tensor)\n",
        "        out = self.fc(fc_input)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0H4z95VyS7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(batch_x, batch_y):\n",
        "    batch = []\n",
        "    for sentence in batch_x:\n",
        "        batch.append(to_index(sentence))\n",
        "    \n",
        "    sen_lengths = th.tensor(list(map(len, batch)))\n",
        "    \n",
        "    batch_tensor = th.zeros((len(batch), sen_lengths.max())).long()\n",
        "    for idx, (sen, sen_len) in enumerate(zip(batch, sen_lengths)):\n",
        "        batch_tensor[idx, :sen_len] = th.LongTensor(sen)\n",
        "        \n",
        "    sen_lengths, sorted_idx = sen_lengths.sort(0, descending=True)\n",
        "    batch_tensor = batch_tensor[sorted_idx]\n",
        "    \n",
        "    batch_labels = th.tensor(batch_y).view(len(batch_y),-1)\n",
        "    batch_labels = batch_labels[sorted_idx]\n",
        "    \n",
        "    return batch_tensor,batch_labels,sen_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KE8GiY9yS7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 10\n",
        "model = RNN(num_voc,40,2,num_layers=2)\n",
        "#model = CBOW_classifier(num_voc, embedding_dim=EMBED_DIM,)\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "print(model)\n",
        "optimizer = th.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01, weight_decay=0.01)\n",
        "#optimizer = th.optim.SGD(model.parameters(),lr=0.01)\n",
        "for e in range(50):\n",
        "    print('epoch: ', e)\n",
        "    total_loss = 0\n",
        "    for i in range(0,len(X_train),BATCH_SIZE):\n",
        "        batch_tensor,batch_labels,sen_lengths = get_batch(X_train[i:i+BATCH_SIZE], Y_train[i:i+BATCH_SIZE])\n",
        "        optimizer.zero_grad()\n",
        "    #         if to_tensor(x).size()[0] <= WINDOW-1:\n",
        "    #             continue\n",
        "        # output of the model\n",
        "        output = model(batch_tensor,sen_lengths)\n",
        "        # compute the loss\n",
        "        loss = criterion(output, batch_labels)\n",
        "        #total_loss += loss.item()\n",
        "        loss.backward()  # compute gradient\n",
        "        #torch.nn.utils.clip_grad_value_(model.parameters(), 5.)  # clip gradient if its norm exceed 5\n",
        "        optimizer.step()  # update parameters\n",
        "    print('  loss: ',total_loss/len(X_train))\n",
        "    print('  acc : ', eva(model,X_val, Y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX0tp-dmyS7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eva(model,x,y):\n",
        "    batch_tensor,batch_labels,sen_lengths = get_batch(x,y)\n",
        "    res = model(batch_tensor,sen_lengths)\n",
        "    return sum(res.argmax(axis=1)==batch_labels).item()/len(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8XXTcXJyS7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNN(num_voc,40,2)\n",
        "#model = CBOW_classifier(num_voc, embedding_dim=EMBED_DIM,)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(model)\n",
        "optimizer = th.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.01, weight_decay=0.01)\n",
        "# training loop\n",
        "model, mean_losss = training_loop(model, optimizer, criterion, EPOCHS, X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3TiFnhCyS7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}